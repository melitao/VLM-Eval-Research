{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f07408-5e0a-4b3b-92d7-eda42e6c2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, I'm an AI language model. I was trained on a large dataset of text to generate coherent and informative responses to a wide range of input prompts. My goal is to assist users in finding the information they need or help them with any tasks that require understanding or generating human-like text. \n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"llava:7b\", messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello, can you explain what you are?\"}\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab99c474-9b1f-48a5-9372-8277b32055b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "  A vision-language model is a type of artificial intelligence that can understand and interpret visual content, such as images or videos, and relate it to textual descriptions or captions. This allows the model to perform tasks like identifying objects within an image, recognizing different scenes or actions, and generating descriptions of what's in the visual content. The model learns by training on large datasets that include both images and their corresponding descriptions. \n"
     ]
    }
   ],
   "source": [
    "# Simple text query\n",
    "response = ollama.chat(model=\"llava:7b\", messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Explain in simple terms what a vision-language model does.\"}\n",
    "])\n",
    "\n",
    "print(\"Model response:\\n\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b7afd3-532b-41c2-b8d1-9ff4a013612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "  The image shows a computer monitor displaying what appears to be a simple graph or chart with three bars of different colors: red, blue, and grey. The graph is set against a background that looks like it could be a window or a screen with text, although the text itself is not clear enough to read. There are also two small icons at the bottom of the image, one on each side, which look like they might be buttons or controls for the application or software running on the computer. The overall appearance suggests this is a screenshot taken from a computer program. \n"
     ]
    }
   ],
   "source": [
    "# Query with image\n",
    "response = ollama.chat(model=\"llava:7b\", messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What do you see in this image?\",\n",
    "        \"images\": [\"data/VLAT/images/100StackedBarChart.png\"]\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Model response:\\n\", response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
