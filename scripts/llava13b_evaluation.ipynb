{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a3eaff-85c6-42aa-8b67-aa8718bffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv, time, random, base64, re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "MODEL_NAME = \"llava:13b\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "USE_CHAT_API = True\n",
    "STREAM = False\n",
    "\n",
    "IMAGE_ROOT = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\data\")\n",
    "VLAT_JSON  = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\data\\VLAT\\vlat_skip_orig.json\")\n",
    "CALVI_JSON = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\data\\CALVI\\calvi_orig.json\")\n",
    "\n",
    "OUT_VLAT = IMAGE_ROOT.parent / \"Output\" / \"VLAT\" / \"Llava13b_Eval\" / \"Random\"\n",
    "OUT_CALVI = IMAGE_ROOT.parent / \"Output\" / \"CALVI\" / \"Llava13b_Eval\" / \"Random\"\n",
    "\n",
    "NUM_RUNS = 10\n",
    "SLEEP_MIN_SEC = 5\n",
    "SLEEP_MAX_SEC = 10\n",
    "BASE_SEED = 12345\n",
    "REQUEST_TIMEOUT = 600\n",
    "\n",
    "VLAT_PROMPT = (\n",
    "\"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer, based only on the chart and not external knowledge. DO NOT GUESS.\n",
    "If you are not sure about your answer or your answer is based on a guess, select \"Omit\".\n",
    "Choose your answer ONLY from the provided options.\"\"\"\n",
    ")\n",
    "\n",
    "CALVI_PROMPT = (\n",
    "\"\"\"I am about to show you an image and ask you a multiple choice question about that image. \n",
    "Please structure your response in the following format:\n",
    "Answer: [Enter the exact text of your chosen option(s)]\n",
    "Explanation: [Provide your reasoning]\n",
    "Select the BEST answer, based only on the chart and not external knowledge.\n",
    "Choose your answer ONLY from the provided options.\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cecb4dd6-1538-4ba9-925d-2662f76a69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunk_slices(total: int, plan: List[Any]) -> List[Tuple[int, int]]:\n",
    "    out, start, remain = [], 0, total\n",
    "    for p in (plan or []):\n",
    "        n = remain if (p == \"rest\") else min(int(p), remain)\n",
    "        if n <= 0:\n",
    "            break\n",
    "        out.append((start, start + n))\n",
    "        start += n\n",
    "        remain -= n\n",
    "        if remain <= 0:\n",
    "            break\n",
    "    if not out and total > 0:\n",
    "        out = [(0, total)]\n",
    "    return out\n",
    "\n",
    "def _sleep_between(min_s=SLEEP_MIN_SEC, max_s=SLEEP_MAX_SEC):\n",
    "    dur = random.randint(int(min_s), int(max_s))\n",
    "    print(f\"Cooling down for {dur} seconds to avoid timeouts...\")\n",
    "    time.sleep(dur)\n",
    "\n",
    "def _fieldnames_for_dataset(name: str) -> List[str]:\n",
    "    base = [\n",
    "        \"id\",\"testname\",\"question\",\"Chart_type\",\"Task\",\n",
    "        \"options\",\"correct_answer\",\"model_answer\",\"is_correct\",\n",
    "        \"image_path\",\"elapsed_seconds\",\"response_raw\"\n",
    "    ]\n",
    "    if name.upper() == \"CALVI\":\n",
    "        i = base.index(\"correct_answer\") + 1\n",
    "        base[i:i] = [\"Misleader\",\"wrong_due_to_misleader\"]\n",
    "    return base\n",
    "\n",
    "def _ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _load_json(path: Path) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _read_image_b64(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bbd5dd-7664-47a0-8a3c-a59e7e463d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _base_prompt(q: Dict[str, Any]) -> str:\n",
    "    opts = q.get(\"options\", [])\n",
    "    letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    lines = [\n",
    "        f\"Question: {q.get('question','').strip()}\",\n",
    "        \"\",\n",
    "        \"Options:\"\n",
    "    ]\n",
    "    for i, opt in enumerate(opts):\n",
    "        lines.append(f\"{letters[i]}. {opt}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _format_prompt(q: Dict[str, Any], prepend: Optional[str] = None) -> str:\n",
    "    core = _base_prompt(q)\n",
    "    if prepend:\n",
    "        return f\"{prepend.strip()}\\n\\n{core}\"\n",
    "    return core\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "def _parse_model_answer(raw: str, options: List[str]) -> str:\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    text = raw.strip()\n",
    "\n",
    "    m = re.search(r\"^answer\\s*:\\s*(.+)$\", text, re.IGNORECASE | re.MULTILINE)\n",
    "    if m:\n",
    "        ans_txt = m.group(1).strip().strip('\"').strip(\"'\")\n",
    "        ans_txt = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", ans_txt).strip()\n",
    "\n",
    "        norm_ans = _normalize(ans_txt)\n",
    "        for opt in options:\n",
    "            if _normalize(opt) == norm_ans:\n",
    "                return opt\n",
    "        for opt in options:\n",
    "            if _normalize(opt) in norm_ans or norm_ans in _normalize(opt):\n",
    "                return opt\n",
    "        if re.fullmatch(r\"[A-Za-z]\", ans_txt):\n",
    "            idx = ord(ans_txt.upper()) - ord('A')\n",
    "            if 0 <= idx < len(options):\n",
    "                return options[idx]\n",
    "        return ans_txt\n",
    "\n",
    "    m2 = re.search(r\"\\b([A-H])\\b\", text, re.IGNORECASE)\n",
    "    if m2:\n",
    "        idx = ord(m2.group(1).upper()) - ord('A')\n",
    "        if 0 <= idx < len(options):\n",
    "            return options[idx]\n",
    "\n",
    "    lowered = _normalize(text)\n",
    "    for opt in options:\n",
    "        if _normalize(opt) in lowered:\n",
    "            return opt\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4ec024-4fcd-478d-9f55-fa799edc6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ollama_chat(prompt: str, images_b64: List[str]) -> str:\n",
    "    url = f\"{OLLAMA_HOST}/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "            \"images\": images_b64 if images_b64 else None\n",
    "        }]\n",
    "    }\n",
    "    if payload[\"messages\"][0][\"images\"] is None:\n",
    "        del payload[\"messages\"][0][\"images\"]\n",
    "    r = requests.post(url, json=payload, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "def _ollama_generate(prompt: str, images_b64: List[str]) -> str:\n",
    "    url = f\"{OLLAMA_HOST}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": images_b64 if images_b64 else None,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    if payload[\"images\"] is None:\n",
    "        del payload[\"images\"]\n",
    "    r = requests.post(url, json=payload, timeout=REQUEST_TIMEOUT)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"response\", \"\")\n",
    "\n",
    "def _resolve_image(img_rel: str) -> Path | None:\n",
    "    img_rel_norm = str(img_rel).replace(\"\\\\\", \"/\")\n",
    "    p = Path(img_rel_norm)\n",
    "    candidates = []\n",
    "\n",
    "    if p.is_absolute():\n",
    "        candidates.append(p)\n",
    "\n",
    "    candidates.append(IMAGE_ROOT / p)\n",
    "\n",
    "    parts = p.parts\n",
    "    name = p.name\n",
    "    dataset = parts[0] if parts else None\n",
    "    if dataset in {\"VLAT\", \"CALVI\"}:\n",
    "        candidates.append(IMAGE_ROOT / dataset / \"images\" / name)\n",
    "\n",
    "    candidates.append(IMAGE_ROOT / \"data\" / p)\n",
    "    if dataset in {\"VLAT\", \"CALVI\"}:\n",
    "        candidates.append(IMAGE_ROOT / \"data\" / dataset / \"images\" / name)\n",
    "\n",
    "    seen = set()\n",
    "    for c in candidates:\n",
    "        if c in seen:\n",
    "            continue\n",
    "        seen.add(c)\n",
    "        if c.exists():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def call_llava(question: Dict[str, Any], prepend_prompt: Optional[str] = None) -> str:\n",
    "    prompt = _format_prompt(question, prepend=prepend_prompt)\n",
    "    images_b64 = []\n",
    "\n",
    "    image_path = question.get(\"image_path\")\n",
    "    if image_path:\n",
    "        resolved = _resolve_image(image_path)\n",
    "        if resolved:\n",
    "            try:\n",
    "                images_b64.append(_read_image_b64(resolved))\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not read image {resolved}: {e}\")\n",
    "        else:\n",
    "            print(f\"[WARN] Image file not found (tried common locations) -> '{image_path}'\")\n",
    "\n",
    "    try:\n",
    "        if USE_CHAT_API:\n",
    "            return _ollama_chat(prompt, images_b64)\n",
    "        else:\n",
    "            return _ollama_generate(prompt, images_b64)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Ollama request failed: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66944888-17bc-4cc2-a923-e3044637d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_row(writer: csv.DictWriter, row: Dict[str, Any], f):\n",
    "    writer.writerow(row)\n",
    "    f.flush()\n",
    "\n",
    "def _eval_one_run(dataset_name: str, qlist: List[Dict[str, Any]], out_csv_path: Path, run_index: int, prepend_prompt: Optional[str] = None):\n",
    "    _ensure_dir(out_csv_path.parent)\n",
    "    fields = _fieldnames_for_dataset(dataset_name)\n",
    "    with open(out_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "        f.flush()\n",
    "\n",
    "        N = len(qlist)\n",
    "        first_batch_n = min(20, N)\n",
    "        batches = _chunk_slices(N, [first_batch_n, \"rest\"])\n",
    "\n",
    "        for bi, (s, e) in enumerate(batches):\n",
    "            batch = qlist[s:e]\n",
    "            for idx, q in enumerate(batch, start=s+1):\n",
    "                t0 = time.time()\n",
    "                raw = call_llava(q, prepend_prompt)\n",
    "                elapsed = time.time() - t0\n",
    "\n",
    "                model_ans = _parse_model_answer(raw, q.get(\"options\", []))\n",
    "                correct = q.get(\"correct_answer\", \"\").strip()\n",
    "\n",
    "                row = {\n",
    "                    \"id\": q.get(\"id\"),\n",
    "                    \"testname\": dataset_name.upper(),\n",
    "                    \"question\": q.get(\"question\"),\n",
    "                    \"Chart_type\": q.get(\"Chart_type\"),\n",
    "                    \"Task\": q.get(\"Task\"),\n",
    "                    \"options\": \"; \".join(q.get(\"options\", [])),\n",
    "                    \"correct_answer\": correct,\n",
    "                    \"model_answer\": model_ans,\n",
    "                    \"is_correct\": str(model_ans.strip() == correct),\n",
    "                    \"image_path\": q.get(\"image_path\", \"\"),\n",
    "                    \"elapsed_seconds\": f\"{elapsed:.3f}\",\n",
    "                    \"response_raw\": raw.strip()\n",
    "                }\n",
    "                if dataset_name.upper() == \"CALVI\":\n",
    "                    row[\"Misleader\"] = q.get(\"Misleader\", \"\")\n",
    "                    row[\"wrong_due_to_misleader\"] = q.get(\"wrong_due_to_misleader\", \"\")\n",
    "\n",
    "                _write_row(writer, row, f)\n",
    "\n",
    "                if s == 0:\n",
    "                    first_done = idx - s\n",
    "                    if first_done == 5:\n",
    "                        if dataset_name.upper() == \"VLAT\":\n",
    "                            print(\"first 5 questions done for vlat\")\n",
    "                        else:\n",
    "                            print(\"first 5 questions done for calvi\")\n",
    "                    elif first_done == 10:\n",
    "                        if dataset_name.upper() == \"VLAT\":\n",
    "                            print(\"first next 5 done\")\n",
    "                        else:\n",
    "                            print(\"first next 5 done (calvi)\")\n",
    "\n",
    "            if bi == 0 and len(batches) > 1:\n",
    "                _sleep_between()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45277e6d-fda4-47b1-9759-993bb30f92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _outdir_for(dataset_name: str) -> Path:\n",
    "    if dataset_name.upper() == \"VLAT\":\n",
    "        return OUT_VLAT\n",
    "    elif dataset_name.upper() == \"CALVI\":\n",
    "        return OUT_CALVI\n",
    "    else:\n",
    "        raise ValueError(\"dataset_name must be 'VLAT' or 'CALVI'\")\n",
    "\n",
    "def run_experiment(dataset_name: str, json_path: Path, prompt_overrides: Optional[str] = None, runs: int = NUM_RUNS):\n",
    "    dataset_name_u = dataset_name.upper()\n",
    "    data = _load_json(json_path)\n",
    "    questions = list(data.get(\"questions\", []))\n",
    "    outdir = _outdir_for(dataset_name_u)\n",
    "    _ensure_dir(outdir)\n",
    "\n",
    "    for run_idx in range(1, runs+1):\n",
    "        if dataset_name_u == \"VLAT\":\n",
    "            seed = BASE_SEED + run_idx\n",
    "        else:\n",
    "            seed = BASE_SEED + 10_000 + run_idx\n",
    "        random.seed(seed)\n",
    "        qlist = questions[:]\n",
    "        random.shuffle(qlist)\n",
    "\n",
    "        if dataset_name_u == \"VLAT\":\n",
    "            out_csv = outdir / f\"vlat_llava13b_run_{run_idx:02d}.csv\"\n",
    "        else:\n",
    "            out_csv = outdir / f\"calvi_llava13b_run_{run_idx:02d}.csv\"\n",
    "\n",
    "        print(f\"Running {dataset_name_u}, run {run_idx:02d} -> {out_csv}\")\n",
    "        _eval_one_run(dataset_name_u, qlist, out_csv, run_idx, prepend_prompt=prompt_overrides)\n",
    "\n",
    "    print(f\"All {dataset_name_u} runs completed.\")\n",
    "\n",
    "def run_all(do_vlat: bool = True, do_calvi: bool = True, runs: int = NUM_RUNS):\n",
    "    if do_vlat:\n",
    "        run_experiment(\"VLAT\", VLAT_JSON, prompt_overrides=VLAT_PROMPT, runs=runs)\n",
    "    if do_calvi:\n",
    "        run_experiment(\"CALVI\", CALVI_JSON, prompt_overrides=CALVI_PROMPT, runs=runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bcd717-3ad8-4d30-868b-691d2095d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running VLAT, run 01 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_01.csv\n",
      "first 5 questions done for vlat\n",
      "first next 5 done\n",
      "Cooling down for 8 seconds to avoid timeouts...\n",
      "Running VLAT, run 02 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_02.csv\n",
      "first 5 questions done for vlat\n",
      "first next 5 done\n",
      "Cooling down for 5 seconds to avoid timeouts...\n",
      "Running VLAT, run 03 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_03.csv\n",
      "first 5 questions done for vlat\n",
      "first next 5 done\n",
      "Cooling down for 10 seconds to avoid timeouts...\n",
      "Running VLAT, run 04 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_04.csv\n",
      "first 5 questions done for vlat\n",
      "first next 5 done\n",
      "Cooling down for 9 seconds to avoid timeouts...\n",
      "Running VLAT, run 05 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_05.csv\n",
      "first 5 questions done for vlat\n",
      "first next 5 done\n",
      "Cooling down for 10 seconds to avoid timeouts...\n",
      "All VLAT runs completed.\n",
      "Running CALVI, run 01 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_01.csv\n",
      "first 5 questions done for calvi\n",
      "first next 5 done (calvi)\n",
      "Cooling down for 5 seconds to avoid timeouts...\n",
      "Running CALVI, run 02 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_02.csv\n",
      "first 5 questions done for calvi\n",
      "first next 5 done (calvi)\n",
      "Cooling down for 6 seconds to avoid timeouts...\n",
      "Running CALVI, run 03 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_03.csv\n",
      "first 5 questions done for calvi\n",
      "first next 5 done (calvi)\n",
      "Cooling down for 8 seconds to avoid timeouts...\n",
      "Running CALVI, run 04 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_04.csv\n",
      "first 5 questions done for calvi\n",
      "first next 5 done (calvi)\n",
      "Cooling down for 10 seconds to avoid timeouts...\n",
      "Running CALVI, run 05 -> C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_05.csv\n",
      "first 5 questions done for calvi\n",
      "first next 5 done (calvi)\n",
      "Cooling down for 10 seconds to avoid timeouts...\n",
      "All CALVI runs completed.\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\"VLAT\",  VLAT_JSON,  prompt_overrides=VLAT_PROMPT,  runs=5)\n",
    "run_experiment(\"CALVI\", CALVI_JSON, prompt_overrides=CALVI_PROMPT, runs=5)\n",
    "\n",
    "# Or both:\n",
    "# run_all(do_vlat=True, do_calvi=True, runs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
