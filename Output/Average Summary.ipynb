{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a9b67e9-4c7a-437f-b2dd-f95a13f70682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files:\n",
      "   vlat_qwen2.5vl_3b_run_01.csv\n",
      "   vlat_qwen2.5vl_3b_run_02.csv\n",
      "   vlat_qwen2.5vl_3b_run_03.csv\n",
      "\n",
      "results dict: {'qwen2.5vl_3b': {1: 23, 2: 25, 3: 23}}\n",
      "\n",
      "VLAT TABLE FOR THIS MODEL:\n",
      "  Model Name  Round 1  Round 2  Round 3   Average\n",
      "qwen2.5vl_3b       23       25       23 23.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# POINT THIS DIRECTLY TO THE 'Random' FOLDER\n",
    "DATA_DIR = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\")\n",
    "\n",
    "# Grab all VLAT run CSVs in that folder\n",
    "filepaths = sorted(DATA_DIR.glob(\"vlat_*_run_*.csv\"))\n",
    "\n",
    "print(\"Found CSV files:\")\n",
    "for fp in filepaths:\n",
    "    print(\"  \", fp.name)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fp in filepaths:\n",
    "    filename = fp.name\n",
    "\n",
    "    # Expect: vlat_qwen2.5vl_3b_run_01.csv\n",
    "    match = re.match(r\"vlat_(.+)_run_(\\d+)\\.csv\", filename)\n",
    "    if not match:\n",
    "        print(f\"Skipping (pattern didn't match): {filename}\")\n",
    "        continue\n",
    "\n",
    "    model_key = match.group(1)      # e.g. \"qwen2.5vl_3b\"\n",
    "    round_num = int(match.group(2)) # e.g. 1, 2, 3\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    if \"is_correct\" not in df.columns:\n",
    "        print(f\"WARNING: 'is_correct' not found in {filename}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    correct_count = df[\"is_correct\"].sum()\n",
    "\n",
    "    if model_key not in results:\n",
    "        results[model_key] = {}\n",
    "\n",
    "    results[model_key][round_num] = correct_count\n",
    "\n",
    "print(\"\\nresults dict:\", results)\n",
    "\n",
    "rows = []\n",
    "for model_key, rounds in results.items():\n",
    "    max_round = max(rounds.keys())\n",
    "    round_scores = [rounds.get(r, 0) for r in range(1, max_round + 1)]\n",
    "    avg_score = sum(round_scores) / len(round_scores)\n",
    "\n",
    "    row = {\"Model Name\": model_key}\n",
    "    for i, score in enumerate(round_scores, start=1):\n",
    "        row[f\"Round {i}\"] = score\n",
    "    row[\"Average\"] = avg_score\n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    round_cols = sorted([c for c in summary_df.columns if c.startswith(\"Round \")]\n",
    "    )\n",
    "    summary_df = summary_df[[\"Model Name\"] + round_cols + [\"Average\"]]\n",
    "    print(\"\\nVLAT TABLE FOR THIS MODEL:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo data found â€“ check that there are vlat_*_run_*.csv files in the Random folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb44c2fb-60d5-4ffb-9f5e-ce8b727dfb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files detected:\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_03.csv\n",
      "\n",
      "Raw results dict:\n",
      "{'gemma3_12b': {1: 25, 2: 26, 3: 23}, 'gemma3_4b': {1: 24, 2: 25, 3: 23}, 'llava13b': {1: 17, 2: 14, 3: 11}, 'llava7b': {1: 14, 2: 16, 3: 19}, 'qwen2.5vl_3b': {1: 23, 2: 25, 3: 23}, 'qwen2.5vl_7b': {1: 19, 2: 19, 3: 19}}\n",
      "\n",
      "===== FINAL VLAT SCORE TABLE =====\n",
      "   Model Name  Round 1  Round 2  Round 3   Average\n",
      "  Gemma 3 12B       25       26       23 24.666667\n",
      "   Gemma 3 4B       24       25       23 24.000000\n",
      "    LLaVA 13B       17       14       11 14.000000\n",
      "     LLaVA 7B       14       16       19 16.333333\n",
      "Qwen 2.5VL 3B       23       25       23 23.666667\n",
      "Qwen 2.5VL 7B       19       19       19 19.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Recursively find ALL run files in any Random/ folder\n",
    "filepaths = list(BASE_DIR.rglob(\"Random/vlat_*_run_*.csv\"))\n",
    "\n",
    "print(\"Files detected:\")\n",
    "for fp in filepaths:\n",
    "    print(\"  \", fp)\n",
    "\n",
    "for fp in filepaths:\n",
    "    filename = fp.name\n",
    "\n",
    "    # Regex for: vlat_<model>_run_<round>.csv\n",
    "    match = re.match(r\"vlat_(.+)_run_(\\d+)\\.csv\", filename)\n",
    "    if not match:\n",
    "        print(f\"Skipping (regex didn't match): {filename}\")\n",
    "        continue\n",
    "\n",
    "    model_key = match.group(1)\n",
    "    round_num = int(match.group(2))\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    if \"is_correct\" not in df.columns:\n",
    "        print(f\"WARNING: is_correct missing in: {filename}\")\n",
    "        continue\n",
    "\n",
    "    correct_count = df[\"is_correct\"].sum()\n",
    "\n",
    "    results.setdefault(model_key, {})[round_num] = correct_count\n",
    "\n",
    "print(\"\\nRaw results dict:\")\n",
    "print(results)\n",
    "\n",
    "# Pretty names for output table\n",
    "pretty_names = {\n",
    "    \"gemma3_4b\": \"Gemma 3 4B\",\n",
    "    \"gemma3_12b\": \"Gemma 3 12B\",\n",
    "    \"llava13b\": \"LLaVA 13B\",\n",
    "    \"llava7b\": \"LLaVA 7B\",\n",
    "    \"qwen2.5vl_3b\": \"Qwen 2.5VL 3B\",\n",
    "    \"qwen2.5vl_7b\": \"Qwen 2.5VL 7B\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for model_key, rounds in results.items():\n",
    "    max_round = max(rounds.keys())\n",
    "    round_scores = [rounds[r] for r in range(1, max_round + 1)]\n",
    "    avg_score = sum(round_scores) / len(round_scores)\n",
    "\n",
    "    row = {\n",
    "        \"Model Name\": pretty_names.get(model_key, model_key),\n",
    "    }\n",
    "\n",
    "    for i, score in enumerate(round_scores, start=1):\n",
    "        row[f\"Round {i}\"] = score\n",
    "\n",
    "    row[\"Average\"] = avg_score\n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# Order columns\n",
    "if not summary_df.empty:\n",
    "    round_cols = sorted([c for c in summary_df.columns if c.startswith(\"Round\")])\n",
    "    summary_df = summary_df[[\"Model Name\"] + round_cols + [\"Average\"]]\n",
    "    summary_df = summary_df.sort_values(\"Model Name\")\n",
    "\n",
    "    print(\"\\n===== FINAL VLAT SCORE TABLE =====\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n No data collected. Check folder structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49841c2-1c90-48bb-bfca-b6f3017891bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files detected:\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_12b_Eval\\Random\\calvi_gemma3_12b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_12b_Eval\\Random\\calvi_gemma3_12b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_12b_Eval\\Random\\calvi_gemma3_12b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_4b_Eval\\Random\\calvi_gemma3_4b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_4b_Eval\\Random\\calvi_gemma3_4b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Gemma3_4b_Eval\\Random\\calvi_gemma3_4b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava13b_Eval\\Random\\calvi_llava13b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava7b_Eval\\Random\\calvi_llava7b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava7b_Eval\\Random\\calvi_llava7b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Llava7b_Eval\\Random\\calvi_llava7b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_3b_Eval\\Random\\calvi_qwen2.5vl_3b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_3b_Eval\\Random\\calvi_qwen2.5vl_3b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_3b_Eval\\Random\\calvi_qwen2.5vl_3b_run_03.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_7b_Eval\\Random\\calvi_qwen2.5vl_7b_run_01.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_7b_Eval\\Random\\calvi_qwen2.5vl_7b_run_02.csv\n",
      "   C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\\Qwen2.5vl_7b_Eval\\Random\\calvi_qwen2.5vl_7b_run_03.csv\n",
      "\n",
      "Raw results dict:\n",
      "{'gemma3_12b': {1: 10, 2: 9, 3: 9}, 'gemma3_4b': {1: 13, 2: 13, 3: 13}, 'llava13b': {1: 13, 2: 11, 3: 13}, 'llava7b': {1: 15, 2: 22, 3: 18}, 'qwen2.5vl_3b': {1: 11, 2: 11, 3: 11}, 'qwen2.5vl_7b': {1: 11, 2: 11, 3: 11}}\n",
      "\n",
      "===== FINAL CALVI SCORE TABLE =====\n",
      "   Model Name  Round 1  Round 2  Round 3   Average\n",
      "  Gemma 3 12B       10        9        9  9.333333\n",
      "   Gemma 3 4B       13       13       13 13.000000\n",
      "    LLaVA 13B       13       11       13 12.333333\n",
      "     LLaVA 7B       15       22       18 18.333333\n",
      "Qwen 2.5VL 3B       11       11       11 11.000000\n",
      "Qwen 2.5VL 7B       11       11       11 11.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\CALVI\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Recursively find ALL run files in any Random/ folder\n",
    "filepaths = list(BASE_DIR.rglob(\"Random/calvi_*_run_*.csv\"))\n",
    "\n",
    "print(\"Files detected:\")\n",
    "for fp in filepaths:\n",
    "    print(\"  \", fp)\n",
    "\n",
    "for fp in filepaths:\n",
    "    filename = fp.name\n",
    "\n",
    "    # Regex for: calvi_<model>_run_<round>.csv\n",
    "    match = re.match(r\"calvi_(.+)_run_(\\d+)\\.csv\", filename)\n",
    "    if not match:\n",
    "        print(f\"Skipping (regex didn't match): {filename}\")\n",
    "        continue\n",
    "\n",
    "    model_key = match.group(1)\n",
    "    round_num = int(match.group(2))\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    if \"is_correct\" not in df.columns:\n",
    "        print(f\"WARNING: is_correct missing in: {filename}\")\n",
    "        continue\n",
    "\n",
    "    correct_count = df[\"is_correct\"].sum()\n",
    "\n",
    "    results.setdefault(model_key, {})[round_num] = correct_count\n",
    "\n",
    "print(\"\\nRaw results dict:\")\n",
    "print(results)\n",
    "\n",
    "# Pretty names for output table\n",
    "pretty_names = {\n",
    "    \"gemma3_4b\": \"Gemma 3 4B\",\n",
    "    \"gemma3_12b\": \"Gemma 3 12B\",\n",
    "    \"llava13b\": \"LLaVA 13B\",\n",
    "    \"llava7b\": \"LLaVA 7B\",\n",
    "    \"qwen2.5vl_3b\": \"Qwen 2.5VL 3B\",\n",
    "    \"qwen2.5vl_7b\": \"Qwen 2.5VL 7B\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for model_key, rounds in results.items():\n",
    "    max_round = max(rounds.keys())\n",
    "    round_scores = [rounds[r] for r in range(1, max_round + 1)]\n",
    "    avg_score = sum(round_scores) / len(round_scores)\n",
    "\n",
    "    row = {\n",
    "        \"Model Name\": pretty_names.get(model_key, model_key),\n",
    "    }\n",
    "\n",
    "    for i, score in enumerate(round_scores, start=1):\n",
    "        row[f\"Round {i}\"] = score\n",
    "\n",
    "    row[\"Average\"] = avg_score\n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# Order columns\n",
    "if not summary_df.empty:\n",
    "    round_cols = sorted([c for c in summary_df.columns if c.startswith(\"Round\")])\n",
    "    summary_df = summary_df[[\"Model Name\"] + round_cols + [\"Average\"]]\n",
    "    summary_df = summary_df.sort_values(\"Model Name\")\n",
    "\n",
    "    print(\"\\n===== FINAL CALVI SCORE TABLE =====\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n No data collected. Check folder structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9916135-de8b-418b-b982-7167d34d72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files detected:\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_03.csv\n",
      "\n",
      "Corrected scores per round (raw dict):\n",
      "{'gemma3_12b': {1: 12.333333333333334, 2: 13.666666666666664, 3: 9.999999999999998}, 'gemma3_4b': {1: 10.833333333333334, 2: 12.166666666666666, 3: 9.499999999999998}, 'llava13b': {1: 5.166666666666669, 2: 1.166666666666667, 3: -1.999999999999999}, 'llava7b': {1: -3.666666666666666, 2: 8.881784197001252e-16, 3: 2.666666666666668}, 'qwen2.5vl_3b': {1: 9.5, 2: 12.166666666666666, 3: 9.5}, 'qwen2.5vl_7b': {1: 14.166666666666666, 2: 14.166666666666666, 3: 14.166666666666668}}\n",
      "\n",
      "===== FINAL VLAT CORRECTED SCORE TABLE =====\n",
      "   Model Name  Round 1  Round 2  Round 3  Average\n",
      "  Gemma 3 12B    12.33    13.67    10.00    12.00\n",
      "   Gemma 3 4B    10.83    12.17     9.50    10.83\n",
      "    LLaVA 13B     5.17     1.17    -2.00     1.44\n",
      "     LLaVA 7B    -3.67     0.00     2.67    -0.33\n",
      "Qwen 2.5VL 3B     9.50    12.17     9.50    10.39\n",
      "Qwen 2.5VL 7B    14.17    14.17    14.17    14.17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ==== 1. Load VLAT metadata (to get number of choices per question) ====\n",
    "\n",
    "VLAT_JSON_PATH = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\data\\VLAT\\vlat_skip.json\")\n",
    "\n",
    "with open(VLAT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    vlat_meta = json.load(f)\n",
    "\n",
    "qid_to_C = {}\n",
    "for q in vlat_meta[\"questions\"]:\n",
    "    qid = q[\"id\"]\n",
    "    # count NON-omit options\n",
    "    non_omit = [opt for opt in q[\"options\"] if opt.lower() != \"omit\"]\n",
    "    qid_to_C[qid] = len(non_omit)\n",
    "\n",
    "choices_df = pd.DataFrame(\n",
    "    [{\"id\": qid, \"C_i\": C_i} for qid, C_i in qid_to_C.items()]\n",
    ")\n",
    "\n",
    "# ==== 2. Compute corrected score for EACH RUN separately ====\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\")\n",
    "\n",
    "# model_key -> {round_num -> corrected_score}\n",
    "per_round_cs = {}\n",
    "\n",
    "filepaths = list(BASE_DIR.rglob(\"Random/vlat_*_run_*.csv\"))\n",
    "print(\"Files detected:\")\n",
    "for fp in filepaths:\n",
    "    print(\" \", fp)\n",
    "\n",
    "for fp in filepaths:\n",
    "    filename = fp.name\n",
    "    match = re.match(r\"vlat_(.+)_run_(\\d+)\\.csv\", filename)\n",
    "    if not match:\n",
    "        print(\"Skipping (pattern mismatch):\", filename)\n",
    "        continue\n",
    "\n",
    "    model_key = match.group(1)\n",
    "    round_num = int(match.group(2))\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # merge per-question choice counts\n",
    "    df = df.merge(choices_df, on=\"id\", how=\"left\")\n",
    "\n",
    "    # drop omits (they get no penalty or reward)\n",
    "    if \"model_answer\" in df.columns:\n",
    "        df_no_omit = df[df[\"model_answer\"].str.lower() != \"omit\"].copy()\n",
    "    else:\n",
    "        df_no_omit = df.copy()\n",
    "\n",
    "    is_correct = df_no_omit[\"is_correct\"].astype(bool)\n",
    "    C_i = df_no_omit[\"C_i\"]\n",
    "\n",
    "    # per-item corrected score\n",
    "    per_item_score = np.where(\n",
    "        is_correct,\n",
    "        1.0,\n",
    "        -1.0 / (C_i - 1)\n",
    "    )\n",
    "\n",
    "    CS = per_item_score.sum()\n",
    "\n",
    "    per_round_cs.setdefault(model_key, {})[round_num] = CS\n",
    "\n",
    "print(\"\\nCorrected scores per round (raw dict):\")\n",
    "print(per_round_cs)\n",
    "\n",
    "# ==== 3. Build table: Model Name | Round 1 | Round 2 | Round 3 | Average ====\n",
    "\n",
    "pretty_names = {\n",
    "    \"gemma3_12b\": \"Gemma 3 12B\",\n",
    "    \"gemma3_4b\": \"Gemma 3 4B\",\n",
    "    \"llava13b\": \"LLaVA 13B\",\n",
    "    \"llava7b\": \"LLaVA 7B\",\n",
    "    \"qwen2.5vl_3b\": \"Qwen 2.5VL 3B\",\n",
    "    \"qwen2.5vl_7b\": \"Qwen 2.5VL 7B\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for model_key, rounds in per_round_cs.items():\n",
    "    # ensure we have them in order 1,2,3 (or however many exist)\n",
    "    max_round = max(rounds.keys())\n",
    "    round_scores = [rounds.get(r, np.nan) for r in range(1, max_round + 1)]\n",
    "\n",
    "    avg_cs = float(np.nanmean(round_scores))\n",
    "\n",
    "    row = {\"Model Name\": pretty_names.get(model_key, model_key)}\n",
    "    for i, score in enumerate(round_scores, start=1):\n",
    "        row[f\"Round {i}\"] = round(score, 2) if pd.notna(score) else np.nan\n",
    "    row[\"Average\"] = round(avg_cs, 2)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "cs_table_df = pd.DataFrame(rows)\n",
    "cs_table_df = cs_table_df.sort_values(\"Model Name\")\n",
    "\n",
    "print(\"\\n===== FINAL VLAT CORRECTED SCORE TABLE =====\")\n",
    "print(cs_table_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a7d35f-dc71-4e7e-bc76-871ec4cc01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files detected:\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_12b_Eval\\Random\\vlat_gemma3_12b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Gemma3_4b_Eval\\Random\\vlat_gemma3_4b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava13b_Eval\\Random\\vlat_llava13b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Llava7b_Eval\\Random\\vlat_llava7b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_3b_Eval\\Random\\vlat_qwen2.5vl_3b_run_03.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_01.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_02.csv\n",
      "  C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\\Qwen2.5vl_7b_Eval\\Random\\vlat_qwen2.5vl_7b_run_03.csv\n",
      "\n",
      "Raw score lists: {'gemma3_12b': [25, 26, 23], 'gemma3_4b': [24, 25, 23], 'llava13b': [17, 14, 11], 'llava7b': [14, 16, 19], 'qwen2.5vl_3b': [23, 25, 23], 'qwen2.5vl_7b': [19, 19, 19]}\n",
      "Corrected score lists: {'gemma3_12b': [12.333333333333334, 13.666666666666664, 9.999999999999998], 'gemma3_4b': [10.833333333333334, 12.166666666666666, 9.499999999999998], 'llava13b': [5.166666666666669, 1.166666666666667, -1.999999999999999], 'llava7b': [-3.666666666666666, 8.881784197001252e-16, 2.666666666666668], 'qwen2.5vl_3b': [9.5, 12.166666666666666, 9.5], 'qwen2.5vl_7b': [14.166666666666666, 14.166666666666666, 14.166666666666668]}\n",
      "\n",
      "===== VLAT REGULAR + CORRECTED TABLE (variable C, omits ignored) =====\n",
      "        Model Score Type  Mean (M)          Range   SD\n",
      "  Gemma 3 12B  Corrected     12.00  (10.0, 13.67) 1.52\n",
      "  Gemma 3 12B    Regular     24.67       (23, 26) 1.25\n",
      "   Gemma 3 4B  Corrected     10.83   (9.5, 12.17) 1.09\n",
      "   Gemma 3 4B    Regular     24.00       (23, 25) 0.82\n",
      "    LLaVA 13B  Corrected      1.44   (-2.0, 5.17) 2.93\n",
      "    LLaVA 13B    Regular     14.00       (11, 17) 2.45\n",
      "     LLaVA 7B  Corrected     -0.33  (-3.67, 2.67) 2.60\n",
      "     LLaVA 7B    Regular     16.33       (14, 19) 2.05\n",
      "Qwen 2.5VL 3B  Corrected     10.39   (9.5, 12.17) 1.26\n",
      "Qwen 2.5VL 3B    Regular     23.67       (23, 25) 0.94\n",
      "Qwen 2.5VL 7B  Corrected     14.17 (14.17, 14.17) 0.00\n",
      "Qwen 2.5VL 7B    Regular     19.00       (19, 19) 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "# ==== 1. Load VLAT question metadata from JSON ====\n",
    "\n",
    "VLAT_JSON_PATH = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\data\\VLAT\\vlat_skip.json\")\n",
    "\n",
    "with open(VLAT_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    vlat_meta = json.load(f)\n",
    "\n",
    "# Build a mapping: question id -> number of NON-OMIT choices (C_i)\n",
    "qid_to_C = {}\n",
    "for q in vlat_meta[\"questions\"]:\n",
    "    qid = q[\"id\"]\n",
    "    options = q[\"options\"]\n",
    "    # treat any option equal to \"omit\" (case-insensitive) as non-choice\n",
    "    non_omit = [opt for opt in options if opt.lower() != \"omit\"]\n",
    "    C_i = len(non_omit)\n",
    "    qid_to_C[qid] = C_i\n",
    "\n",
    "# Put into a DataFrame for easy merge\n",
    "choices_df = pd.DataFrame(\n",
    "    [{\"id\": qid, \"C_i\": C_i} for qid, C_i in qid_to_C.items()]\n",
    ")\n",
    "\n",
    "# ==== 2. Walk all VLAT run CSVs and compute scores ====\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\Melita\\CSE 4001\\VLM-Eval-Research\\Output\\VLAT\")\n",
    "\n",
    "raw_scores = {}   # model_key -> list of raw scores (R)\n",
    "corr_scores = {}  # model_key -> list of corrected scores (CS)\n",
    "\n",
    "filepaths = list(BASE_DIR.rglob(\"Random/vlat_*_run_*.csv\"))\n",
    "\n",
    "print(\"Files detected:\")\n",
    "for fp in filepaths:\n",
    "    print(\" \", fp)\n",
    "\n",
    "for fp in filepaths:\n",
    "    filename = fp.name\n",
    "    match = re.match(r\"vlat_(.+)_run_(\\d+)\\.csv\", filename)\n",
    "    if not match:\n",
    "        print(\"Skipping (pattern mismatch):\", filename)\n",
    "        continue\n",
    "\n",
    "    model_key = match.group(1)\n",
    "    run_num = int(match.group(2))\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # Merge in C_i (per-question number of choices)\n",
    "    df = df.merge(choices_df, on=\"id\", how=\"left\")\n",
    "\n",
    "    # --- Regular raw score: simply count correct answers ---\n",
    "    R_raw = df[\"is_correct\"].sum()\n",
    "\n",
    "    # --- Corrected score: per-question scoring with variable C_i ---\n",
    "    # Ignore omits completely\n",
    "    if \"model_answer\" in df.columns:\n",
    "        df_no_omit = df[df[\"model_answer\"].str.lower() != \"omit\"].copy()\n",
    "    else:\n",
    "        df_no_omit = df.copy()  # if no explicit omit flag\n",
    "\n",
    "    # Per-question score:\n",
    "    # correct -> +1\n",
    "    # incorrect -> -1/(C_i - 1)\n",
    "    # (C_i is number of non-omit choices for that question)\n",
    "    is_correct = df_no_omit[\"is_correct\"].astype(bool)\n",
    "    C_i = df_no_omit[\"C_i\"]\n",
    "\n",
    "    per_item_score = np.where(\n",
    "        is_correct,\n",
    "        1.0,\n",
    "        -1.0 / (C_i - 1)\n",
    "    )\n",
    "\n",
    "    CS = per_item_score.sum()\n",
    "\n",
    "    raw_scores.setdefault(model_key, []).append(R_raw)\n",
    "    corr_scores.setdefault(model_key, []).append(CS)\n",
    "\n",
    "print(\"\\nRaw score lists:\", raw_scores)\n",
    "print(\"Corrected score lists:\", corr_scores)\n",
    "\n",
    "# ==== 3. Build final table like Table 2 (Regular + Corrected) ====\n",
    "\n",
    "pretty_names = {\n",
    "    \"gemma3_12b\": \"Gemma 3 12B\",\n",
    "    \"gemma3_4b\": \"Gemma 3 4B\",\n",
    "    \"llava13b\": \"LLaVA 13B\",\n",
    "    \"llava7b\": \"LLaVA 7B\",\n",
    "    \"qwen2.5vl_3b\": \"Qwen 2.5VL 3B\",\n",
    "    \"qwen2.5vl_7b\": \"Qwen 2.5VL 7B\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "def add_rows(score_dict, score_type):\n",
    "    for model_key, scores in score_dict.items():\n",
    "        scores = list(scores)\n",
    "        mean_val = float(np.mean(scores))\n",
    "        sd_val = float(np.std(scores, ddof=0))\n",
    "        smin, smax = min(scores), max(scores)\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": pretty_names.get(model_key, model_key),\n",
    "            \"Score Type\": score_type,\n",
    "            \"Mean (M)\": round(mean_val, 2),\n",
    "            \"Range\": f\"({round(smin, 2)}, {round(smax, 2)})\",\n",
    "            \"SD\": round(sd_val, 2),\n",
    "        })\n",
    "\n",
    "# Regular row (raw score) and Corrected row\n",
    "add_rows(raw_scores, \"Regular\")\n",
    "add_rows(corr_scores, \"Corrected\")\n",
    "\n",
    "table_df = pd.DataFrame(rows)\n",
    "table_df = table_df.sort_values([\"Model\", \"Score Type\"])\n",
    "\n",
    "print(\"\\n===== VLAT REGULAR + CORRECTED TABLE (variable C, omits ignored) =====\")\n",
    "print(table_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
